# Copyright (C) 2018 NVIDIA Corporation.  All rights reserved.
# Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).

# logger options
image_save_iter: 10000000     # How often do you want to save output images during training
image_display_iter: 10       # How often do you want to display output images during training
display_size: 8               # How many images do you want to display each time
snapshot_save_iter: 5000      # How often do you want to save trained models
log_iter: 1                   # How often do you want to log the training stats

# optimization options
max_iter: 1000000             # maximum number of training iterations
batch_size: 1                 # batch size
weight_decay: 0.0001          # weight decay
beta1: 0.5                    # Adam parameter
beta2: 0.999                  # Adam parameter
init: kaiming                 # initialization [gaussian/kaiming/xavier/orthogonal]
lr: 0.0001                    # initial learning rate
lr_policy: step               # learning rate scheduler
step_size: 100000             # how often to decay learning rate
gamma: 0.5                    # how much to decay learning rate
gan_w: 3                      # weight of adversarial loss
recon_x_w: 12                 # weight of image reconstruction loss
recon_s_w: 1                  # weight of style reconstruction loss
recon_c_w: 2                  # weight of content reconstruction loss
recon_x_cyc_w: 12             # weight of explicit style augmented cycle consistency loss
vgg_w: 0                      # weight of domain-invariant perceptual loss
ratio_disc_gen: 5             # ratio training discriminator vs generator 5 means 5 update of the discriminator for one of the generator

ratio_disc_gen_global: 5     # ratio training discriminator vs generator (global training)

train_global: 1               # if set to 1 train both network MUNIT and G2

## HD ##
recon_x_w_HD: 10              # weight of image reconstruction loss hd
train_G2_only: -1             # if set to 1 train G2 only
gan_w_HD: 3                   # weight of adversarial loss for G2
recon_synth_w: 0

semantic_w: 3                 # weight of semantic conservation loss
recon_mask: 1                 # default 0 do not touch recon loss, 1 do not compute cycle consistency loss on masked region
domain_adv_w: 0 

synthetic_frequency: -1       # frequency to which we show synthetic examples -1 if we don't want to
#classifier ckpt path: 
class_ckpt_path: /network/home/cosnegau/ckpt_small/resnet-18-epoch24.pth

# Semantic segmentation ckpt path:
semantic_ckpt_path: /network/tmp1/ccai/checkpoints/roadSegmentation/resnet_34_8s_cityscapes_best.pth

# model options
gen_state: 1                  # Default 0, 1 means using one common style encoder, 2 one autoencoder only
guided: 1                     # Default 0 random style picked (multi modal), 1 means guided training

# FID 
batch_size_fid: 0            # batch_size to infer the model when computing fid
eval_fid: 0                   # Default 0, 1 means we track FID during training 

# Path to the inception moment computed on the real dataset of 900 flooded images
inception_moment_path: /network/tmp1/ccai/data/munit_dataset/inception_moments.npz
      

gen:
  dim: 64                     # number of filters in the bottommost layer
  mlp_dim: 256                # number of filters in MLP
  style_dim: 16               # length of style code
  activ: relu                 # activation function [relu/lrelu/prelu/selu/tanh]
  n_downsample: 2             # number of downsampling layers in content encoder
  n_res: 4                    # number of residual blocks in content encoder/decoder
  pad_type: reflect           # padding type [zero/reflect]
dis:
  dim: 64                     # number of filters in the bottommost layer
  norm: none                  # normalization layer [none/bn/in/ln]
  activ: lrelu                # activation function [relu/lrelu/prelu/selu/tanh]
  n_layer: 4                  # number of layers in D
  gan_type: lsgan             # GAN loss [lsgan/nsgan]
  num_scales: 3               # number of scales
  pad_type: reflect           # padding type [zero/reflect]

# data options
input_dim_a: 3                              # number of image channels [1/3]
input_dim_b: 3                              # number of image channels [1/3]
num_workers: 8                              # number of data loading threads
new_size: 256                               # first resize the shortest image side to this size
new_size_HD: 512                            # first resize the shortest image side to this size
crop_image_height: 256                      # random crop image of this height
crop_image_width: 256                       # random crop image of this width

data_folder_train_a: /network/home/cosnegau/MUNIT/trainA/
data_list_train_a: /network/tmp1/ccai/data/munit_dataset/trainA.txt
data_folder_test_a: /network/home/cosnegau/MUNIT/testA/
data_list_test_a: /network/tmp1/ccai/data/munit_dataset/testA.txt
data_folder_train_b: /network/home/cosnegau/MUNIT/trainB/
data_list_train_b: /network/tmp1/ccai/data/munit_dataset/trainB.txt
data_folder_test_b: /network/home/cosnegau/MUNIT/testB/
data_list_test_b: /network/tmp1/ccai/data/munit_dataset/testB.txt
data_list_train_a_seg: /network/tmp1/ccai/data/munit_dataset/trainA_seg.txt
data_list_train_b_seg: /network/tmp1/ccai/data/munit_dataset/trainB_seg.txt
data_list_train_a_synth: /network/tmp1/ccai/data/munit_dataset/trainA_synth.txt
data_list_train_b_synth: /network/tmp1/ccai/data/munit_dataset/trainB_synth.txt
data_list_train_b_seg_synth: /network/tmp1/ccai/data/munit_dataset/trainB_seg_synth.txt
# list image for FID monitoring
data_list_fid_a: /network/tmp1/ccai/data/munit_dataset/trainA_fid.txt
data_list_fid_b: /network/tmp1/ccai/data/munit_dataset/trainB.txt
